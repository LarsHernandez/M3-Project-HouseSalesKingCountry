---
title: "SDS 2019 - M3: Group Assignment"
author: "Andreas, Simon, Jess, Lars"
date: "14/9/2019"
output:
  html_document:
    code_folding: hide
    theme: flatly
    toc: yes
    toc_float:
      collapsed: no
---

# House prices King County

```{r message=FALSE, warning=FALSE}
#devtools::install_github("thomasp85/patchwork")
library(tidyverse)
library(keras)
library(caret)
library(patchwork)
library(knitr)
library(kableExtra)
library(ggmap)
library(tidymodels)
```


```{r message=FALSE, warning=FALSE}
kc <- read_csv("kc_house_data.csv")

kc$cluster <- kmeans(cbind(kc$lat, kc$long), centers = 100, nstart = 10, iter.max = 50)$cluster
```

```{r}
head(kc)
```
















## Data exploration

```{r fig.height=4, fig.width=12}
range(kc$date)
difftime(range(kc$date)[1],range(kc$date)[2])

p1 <- kc %>% ggplot(aes(price))       + geom_histogram(bins=200) + labs(title = "Price distribution", y=NULL, x=NULL)
p2 <- kc %>% ggplot(aes(date))        + geom_histogram(bins=56)  + labs(title = "Date of sale", y=NULL, x=NULL)
p3 <- kc %>% ggplot(aes(yr_built))    + geom_histogram(bins=115) + labs(title = "Date of construction", y=NULL, x=NULL)
p4 <- kc %>% ggplot(aes(grade))       + geom_bar()               + labs(title = "House grade", y=NULL, x=NULL)
p5 <- kc %>% ggplot(aes(sqft_living)) + geom_histogram(bins=200) + labs(title = "House Squarefeet", y=NULL, x=NULL)
p6 <- kc %>% ggplot(aes(round(bathrooms))) + geom_bar()          + labs(title = "House bathrooms", y=NULL, x=NULL)
p7 <- kc %>% ggplot(aes(round(condition))) + geom_bar()          + labs(title = "House condition", y=NULL, x=NULL)
p8 <- kc %>% ggplot(aes(as.factor(waterfront)))+ geom_bar()               + labs(title = "House waterfront", y=NULL, x=NULL)
p9 <- kc %>% ggplot(aes(round(view))) + geom_bar()               + labs(title = "House view", y=NULL, x=NULL)

p2 + p3 + p1 + (p6 + p7) + (p8 + p9) + (p4 + p5)
```


```{r fig.height=5, fig.width=10}
d1 <- kc %>% group_by(zipcode) %>% summarize(mean = mean(price)/1000) %>% 
  ggplot(aes(reorder(as.factor(zipcode),mean), mean)) + geom_col() + theme(axis.text.x = element_text(angle = 90)) +
  labs(title="Price by zipcode", x=NULL, y=NULL)
d2 <- kc %>% group_by(view) %>% summarize(mean = mean(price)/1000) %>% 
  ggplot(aes(reorder(as.factor(view),mean), mean)) + geom_col() +
  labs(title="Price by view", x=NULL, y=NULL)
d3 <- kc %>% group_by(bedrooms) %>% summarize(mean = mean(price)/1000) %>% 
  ggplot(aes(reorder(as.factor(bedrooms),mean), mean)) + geom_col() +
  labs(title="Price by number of bedrooms", x=NULL, y=NULL)
d4 <- kc %>% group_by(waterfront) %>% summarize(mean = mean(price)/1000) %>% 
  ggplot(aes(reorder(as.factor(waterfront),mean), mean)) + geom_col() +
  labs(title="Price by waterfront", x=NULL, y=NULL)
d5 <- kc %>% group_by(condition) %>% summarize(mean = mean(price)/1000) %>% 
  ggplot(aes(reorder(as.factor(condition),mean), mean)) + geom_col() +
  labs(title="Price by condition", x=NULL, y=NULL)
d6 <- kc %>% group_by(grade) %>% summarize(mean = mean(price)/1000) %>% 
  ggplot(aes(reorder(as.factor(grade),mean), mean)) + geom_col() +
  labs(title="Price by grade", x=NULL, y=NULL)
d7 <- kc %>% group_by(yr_built) %>% summarize(mean = mean(price)/1000) %>% 
  ggplot(aes(as.numeric(yr_built), mean)) + geom_area() +
  labs(title="Price by year build", x=NULL, y=NULL)
d8 <- kc %>% group_by(date) %>% summarize(mean = mean(price)/1000) %>% 
  ggplot(aes(date, mean)) + geom_point() + geom_smooth() +
  labs(title="Price by date of sale", x=NULL, y=NULL)

(d1 / d3 / d7)
(d4 | d5 | d2) / (d6 | d8)
```




```{r fig.height=6, fig.width=9, message=FALSE, warning=FALSE}
lat <- range(kc$lat)
lon <- range(kc$long)

coord <- as.matrix(data.frame(min = c(lon[1]-0.25, lat[1]-0.05), 
                              max = c(lon[2]+0.05, lat[2]+0.05), 
                              row.names = c("x","y")))

map13 <- get_stamenmap(coord, zoom = 10, maptype = "toner-lite", force=TRUE)

high <- "#084081"
low <- "#252525"

ggmap(map13) +
  labs(title="Map of King County", subtitle="Empty map from stamen-map", x=NULL, y=NULL)

ggmap(map13) +
  stat_density_2d(data=kc, aes(long,lat, fill=..level..), geom = "polygon", alpha = .7) + 
  labs(title="Map of King County - density", subtitle = "21.613 housing sales in 2014 - 2016", fill="Density of\nsales", x=NULL, y=NULL)

ggmap(map13) +
  geom_point(data = kc, aes(long,lat, color=price/1000), alpha = 0.8) + 
  labs(title="Map of King County - price", subtitle = "The mean price of all sales is 540.088, and median is 450.000", color="Price in\n1000 USD", x=NULL, y=NULL)


s1 <- ggmap(map13) +
  geom_point(data = kc, aes(long,lat, fill=as.factor(zipcode)), alpha = 0.1, size=3, show.legend = F, color="black", shape=21) + 
  labs(title="Map of King County - zipcodes", subtitle = "Sales grouped by zipcodes", x=NULL, y=NULL)

s2 <- ggmap(map13) +
  geom_point(data = kc, aes(long,lat, fill=as.factor(cluster)), alpha = 0.1, size=3, show.legend = F, color="black", shape=21) + 
  labs(title="Map of King County - clusters", subtitle = "Sales grouped by kmeans clusters", x=NULL, y=NULL)

s1 + s2
```





```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
coord <- as.matrix(data.frame(min = c(lon[1], lat[1]), 
                              max = c(lon[2]-0.5, lat[2]), 
                              row.names = c("x","y")))

map13 <- get_stamenmap(coord, zoom = 10, maptype = "toner-lite", force=TRUE)

kcc     <- kc
kcc$cut <- cut(log(kcc$price), 15)
kcc     <- kcc %>% group_by(cut) %>% mutate(mean = round(mean(price),-3))

ggmap(map13) +
  stat_density_2d(data=kcc, aes(long,lat, fill=..level..), geom = "polygon", alpha = .3, show.legend = F) +
  geom_point(data=kcc, aes(long,lat), alpha=0.1, size=0.5) +
  scale_fill_viridis_c(option="cividis", end = 0.7) +
  facet_wrap(~mean, nrow=3) + 
  labs(title="Price cut into 15 chunks", x=NULL, y=NULL)
```





```{r}
skc <- kc %>% select(-id, -date) %>% 
  filter(bedrooms != 33)

index    <- createDataPartition(skc$price, p = 0.8, list = FALSE)
training <- skc[index,] 
test     <- skc[-index,] 

paste0("The training set has ",  dim(training)[1], 
       " And the test set has ", dim(test)[1], " observations")

model_recipe <- recipe(price ~ ., data = training)

model_recipe_steps <- model_recipe %>% 
  step_num2factor(yr_built, view, waterfront, floors, yr_built, yr_renovated, cluster, zipcode) %>% 
  step_dummy(yr_built, view, waterfront, floors, yr_built, yr_renovated, cluster, zipcode) %>% 
  step_range(bedrooms, bathrooms, sqft_living, sqft_lot, condition, grade, sqft_above, 
             sqft_basement, sqft_living15, sqft_lot15, min = 0, max = 1)

prepped_recipe <- prep(model_recipe_steps, training = training)

training <- bake(prepped_recipe, training) 
test     <- bake(prepped_recipe, test)
test[is.na(test)] <- 0
x_train  <- training %>% select(-price)
x_test   <- test     %>% select(-price)
y_train  <- training %>% select(price)
y_test   <- test     %>% select(price)
```



```{r}
cv <- trainControl(method = "cv", number = 5)

t0 <- Sys.time()

fit_glm <- train(price     ~ .,
                 data      = training,
                 trControl = cv, 
                 method    = "lm", 
                 metric    = "RMSE")

t1 <- Sys.time()

fit_lar <- train(price     ~ .,
                 data      = training,
                 trControl = cv, 
                 method    = 'lars',
                 tuneGrid  = expand.grid(.fraction=seq(.01,.99,length=50)),
                 metric    = "RMSE")

t2 <- Sys.time()

fit_ela <- train(price     ~ .,
                 data      = training,
                 trControl = cv, 
                 tuneGrid  = expand.grid(alpha  = seq(0, 1,    by = 0.1),
                                         lambda = seq(1, 1000, by = 100)),
                 method    = "glmnet", 
                 family    = "gaussian",
                 metric    = "RMSE")

t3 <- Sys.time()

fit_tre <- train(price ~ .,
                 data      = training,
                 trControl = cv, 
                 method    = "rpart", 
                 metric    = 'RMSE')

t4 <- Sys.time()

fit_raf <- train(price     ~ ., 
                 data      = training,
                 trControl = cv,
                 tuneGrid  = expand.grid(.mtry = (1)),
                 method    = 'rf',
                 metric    = 'RMSE')

t5 <- Sys.time()

model <- keras_model_sequential() %>% 
  layer_dense(input_shape = dim(x_train)[2], units = 128, activation = "relu") %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dropout(0.3) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(1)
model %>%  compile(
  loss = "mae", 
  optimizer = "adam",
  metrics = list("mean_absolute_error"))
model %>% fit(
  as.matrix(x_train),
  as.matrix(y_train),
  epochs = 100,
  batch_size = 32,
  validation_split = 0.1)
pred <- predict(model, x = as.matrix(x_test))

t6 <- Sys.time()

fit_xgb <- train(price     ~ .,
                 data      = training,
                 trControl = cv, 
                 method    = "xgbTree",
                 metric    = "RMSE")

t7 <- Sys.time()


fit_nne <- train(price     ~ .,
                 data      = training,
                 trControl = trainControl(method="repeatedcv", repeats = 2, number = 5), 
                 method    = "nnet",
                 tuneGrid  = expand.grid(size = c(1,3,5), decay = c(0.1, 0.3)),
                 MaxNWts   = 10000,
                 verbose   = F,
                 metric    = "RMSE", 
                 linout    = 1)

t8 <- Sys.time()

method="nnet"
```




```{r}
cat(paste0("Size of training set: ", nrow(training)," of 21613\n","\n",
           "LM:             ", round(difftime(t1,t0, units = "mins"),2),"\n",
           "LARS:           ", round(difftime(t2,t1, units = "mins"),2),"\n",
           "Elastic:        ", round(difftime(t3,t2, units = "mins"),2),"\n",
           "Tree:           ", round(difftime(t4,t3, units = "mins"),2),"\n",
           "Random Forrest: ", round(difftime(t5,t4, units = "mins"),2),"\n",
           "Neural Net:     ", round(difftime(t6,t5, units = "mins"),2),"\n",
           "XGB:            ", round(difftime(t7,t6, units = "mins"),2),"\n",
           "Total:          ", round(difftime(t7,t0, units = "mins"),2),"\n"))
```

```{r}
RMSE = function(m, o){
  sqrt(mean((m - o)^2))
}

MAE = function(m, o){
  mean(abs(m - o))
}

pred_glm <- predict(fit_glm,  newdata=test) %>% as.vector
mae_glm <- MAE(pred_glm, test$price)

pred_lar <- predict(fit_lar,  newdata=test) %>% as.vector
mae_lar <- MAE(pred_lar, test$price)

pred_ela <- predict(fit_ela,  newdata=test) %>% as.vector
mae_ela <- MAE(pred_ela, test$price)

pred_tre <- predict(fit_tre,  newdata=test) %>% as.vector
mae_tre <- MAE(pred_tre, test$price)

pred_raf <- predict(fit_raf,  newdata=test) %>% as.vector
mae_raf <- MAE(pred_raf, test$price)

mae_nnt <- MAE(pred, test$price)

random   <- rep(mean(training$price), nrow(test))
mae_ran <- MAE(random, test$price)

pred_xgb <- predict(fit_xgb,  newdata=test) %>% as.vector
mae_xgb <- MAE(pred_xgb, test$price)


pred_nne <- predict(fit_nne,  newdata=test) %>% as.vector
mae_nne <- MAE(pred_nne, test$price)

kable(cbind(mae_glm, mae_lar, mae_ela, mae_tre, mae_raf, mae_nnt, mae_ran, mae_xgb)) %>% 
  kable_styling("bordered", "condensed")

rbind(mae_glm, mae_lar, mae_ela, mae_tre, mae_raf, mae_nnt, mae_ran, mae_xgb) %>% 
  as_tibble %>% 
  rename(mae = V1) %>% 
  mutate(Model = c("LinearModel", "LARS","ElasticNet", "RegressionTree", "RandomForrest", "NeuralNet","RandomAssignment", "XGB")) %>% 
  ggplot(aes(reorder(Model, desc(mae)), mae/1000)) + 
  geom_col() +
  geom_text(aes(y=(mae/1000)-10, label=round(mae/1000,1)), color="white") +
  labs(title="Model performance Mean Absolute Error", subtitle="All models are run with same train/test split",
       y="MAE", x=NULL) + 
  coord_flip()
```




















## JESS NET

```{r}
x_train <- tra %>% select(-price)
y_train <- tra %>% select(price)

x_test <- tes %>% select(-price)
y_test <- tes %>% select(price)

zip_train <- training$zipcode %>% as.character() %>% as.numeric()
zip_test <- test$zipcode %>% as.character() %>% as.numeric()

zipp <- tibble(z = zip_train) %>% mutate(z=as.factor(z)) %>% mutate(z=as.numeric(z))
zip_train <- zipp$z

main_input <- layer_input(shape = c(1), dtype = 'int32', name = 'main_input')

lstm_out <- main_input %>% 
  layer_embedding(input_dim = 200, output_dim = 5, input_length = 1) %>% 
    layer_flatten()

auxiliary_input <- layer_input(shape = c(dim(x_train)[2]), name = 'aux_input')

main_output <- layer_concatenate(c(lstm_out, auxiliary_input)) %>%  
  layer_dense(units = 64, activation = 'relu') %>% 
  layer_dropout(0.3) %>%
  layer_dense(units = 64, activation = 'relu') %>% 
  layer_dropout(0.2) %>%
  layer_dense(units = 1)

model <- keras_model(
  inputs = c(main_input, auxiliary_input), 
  outputs = c(main_output)
)

model %>%  compile(
  loss = "mse", 
  optimizer = "adam",
  metrics = "mean_absolute_error")

model %>% fit(
  x = list(as.matrix(zip_train), as.matrix(x_train)),
  y = as.matrix(y_train),
  epochs = 10,
  batch_size = 50,
  validation_split = 0.1)
```
















